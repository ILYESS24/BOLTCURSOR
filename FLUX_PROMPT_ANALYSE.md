# üîç ANALYSE DU FLUX : QUAND VOUS ENVOYEZ UN PROMPT

## üìç √âTAPE 1 : PAGE D'ACCUEIL (`_index.tsx`)

**Fichier :** `app/routes/_index.tsx`

**Ce qui se passe :**
1. Vous tapez votre prompt dans le `textarea` (ligne 97-103)
2. Vous cliquez sur le bouton "ArrowUp" ou appuyez sur **Enter** (ligne 52-57)
3. La fonction `handleSubmit()` est appel√©e (ligne 43-48)

```typescript
const handleSubmit = () => {
  if (prompt.trim()) {
    // Redirige vers /chat avec le message et le mod√®le s√©lectionn√©
    navigate(`/chat?q=${encodeURIComponent(prompt)}&model=${encodeURIComponent(selectedModel)}`);
  }
};
```

**R√©sultat :** Navigation vers `/chat?q=VOTRE_PROMPT&model=MODEL_ID`

---

## üìç √âTAPE 2 : ROUTE CHAT (`chat.tsx`)

**Fichier :** `app/routes/chat.tsx`

### 2.1 Loader (c√¥t√© serveur)
```typescript
export async function loader({ request }: LoaderFunctionArgs) {
  const url = new URL(request.url);
  const q = url.searchParams.get('q');        // Votre prompt
  const model = url.searchParams.get('model') || DEFAULT_MODEL;  // Mod√®le s√©lectionn√©
  return json({ initialMessage: q || null, initialModel: model });
}
```

### 2.2 Composant Chat (c√¥t√© client)

**Quand le composant charge :**
1. Il r√©cup√®re `initialMessage` et `initialModel` depuis le loader (ligne 19)
2. Un `useEffect` d√©tecte le message initial (lignes 48-64) :

```typescript
useEffect(() => {
  if (initialMessage && messages.length === 0) {
    // Active le chat
    chatStore.setKey('started', true);
    chatStore.setKey('showChat', true);
    
    // Ajoute votre message √† l'interface
    const userMessage = { role: 'user', content: initialMessage };
    setMessages([userMessage]);
    
    // ENVOIE LA REQU√äTE AU BACKEND
    setIsLoading(true);
    fetcher.submit(
      { message: initialMessage, model: selectedModel },
      { method: 'POST', action: '/api/chat' }  // ‚Üê Appel API
    );
  }
}, [initialMessage]);
```

**R√©sultat :** 
- Votre message appara√Æt dans l'interface
- Une requ√™te POST est envoy√©e √† `/api/chat` avec votre message et le mod√®le

---

## üìç √âTAPE 3 : API CHAT (`api.chat.ts`)

**Fichier :** `app/routes/api.chat.ts`

### 3.1 R√©ception de la requ√™te (ligne 12)
```typescript
export async function action({ request, context }: ActionFunctionArgs) {
  const env = context?.cloudflare?.env;  // Variables d'environnement
```

### 3.2 V√©rifications (lignes 18-57)
- ‚úÖ V√©rifie que les cl√©s API sont configur√©es
- ‚úÖ Parse le JSON de la requ√™te
- ‚úÖ Extrait `message` et `model` du body
- ‚úÖ V√©rifie le cache (si r√©ponse d√©j√† en cache, la retourne)

### 3.3 Pr√©paration des messages (lignes 76-86)
```typescript
const messages = [
  {
    role: 'system',
    content: `Tu es un assistant IA intelligent et utile...`
  },
  {
    role: 'user',
    content: message  // Votre prompt
  }
];
```

### 3.4 Appel au service IA (lignes 91-96)
```typescript
aiResponse = await aiService.chat({
  messages,
  model,              // Le mod√®le s√©lectionn√© (ex: "openrouter/claude-3-5-sonnet")
  temperature: 0.7,
  maxTokens: 4000
}, env);
```

### 3.5 Retour de la r√©ponse (ligne 165)
```typescript
return json({
  response: aiResponse.content,    // La r√©ponse de l'IA
  model: aiResponse.model,
  usage: aiResponse.usage,          // Tokens utilis√©s
  cost: aiResponse.cost,            // Co√ªt estim√©
  timestamp: aiResponse.timestamp,
  requestId,
  cached: false
});
```

---

## üìç √âTAPE 4 : SERVICE IA (`ai-service.ts`)

**Fichier :** `app/lib/ai-service.ts`

### 4.1 M√©thode `chat()` (ligne 58)
```typescript
public async chat(request: ChatRequest): Promise<ChatResponse> {
  const model = request.model || 'gpt-4';
  const aiModel = getModelById(model);  // R√©cup√®re la config du mod√®le
  
  // D√©termine le provider (openrouter dans notre cas)
  if (aiModel.provider === 'openrouter') {
    return await this.chatWithOpenRouter(request, aiModel);
  }
}
```

### 4.2 Appel OpenRouter (`chatWithOpenRouter`) (lignes 384-443)

**Requ√™te HTTP vers OpenRouter :**
```typescript
const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${this.openrouterApiKey}`,  // Votre cl√© API
    'Content-Type': 'application/json',
    'HTTP-Referer': 'https://bolt.new',
    'X-Title': 'Bolt.new AI Assistant'
  },
  body: JSON.stringify({
    model: model.id,              // Ex: "openrouter/claude-3-5-sonnet"
    messages: request.messages,    // Votre prompt + system message
    temperature: 0.7,
    max_tokens: 4000,
    stream: false
  })
});
```

**Traitement de la r√©ponse :**
```typescript
const data = await response.json();

return {
  content: data.choices[0].message.content,  // La r√©ponse de l'IA
  model: model.id,
  usage: {
    promptTokens: data.usage?.prompt_tokens || 0,
    completionTokens: data.usage?.completion_tokens || 0,
    totalTokens: data.usage?.total_tokens || 0
  },
  cost: estimateCost(...),  // Co√ªt calcul√©
  timestamp: new Date().toISOString()
};
```

---

## üìç √âTAPE 5 : RETOUR AU CLIENT (`chat.tsx`)

**Fichier :** `app/routes/chat.tsx`

### 5.1 R√©ception de la r√©ponse (lignes 67-78)
```typescript
useEffect(() => {
  if (fetcher.data && fetcher.state === 'idle') {
    setIsLoading(false);
    if (fetcher.data.response) {
      const assistantMessage = { 
        role: 'assistant', 
        content: fetcher.data.response  // La r√©ponse de l'IA
      };
      setMessages(prev => [...prev, assistantMessage]);  // Ajoute √† l'interface
    }
  }
}, [fetcher.data, fetcher.state]);
```

### 5.2 Affichage (lignes 100-115)
```typescript
{messages.map((msg, idx) => (
  <div className={`flex ${msg.role === 'user' ? 'justify-end' : 'justify-start'}`}>
    <div className="max-w-3xl rounded-lg p-4">
      {msg.content}  {/* Affiche le contenu */}
    </div>
  </div>
))}
```

---

## üîÑ R√âSUM√â DU FLUX COMPLET

```
1. _index.tsx
   ‚îî‚îÄ> Vous tapez un prompt
   ‚îî‚îÄ> handleSubmit() ‚Üí navigate('/chat?q=...&model=...')

2. chat.tsx (loader)
   ‚îî‚îÄ> Extrait q et model de l'URL
   ‚îî‚îÄ> Retourne { initialMessage, initialModel }

3. chat.tsx (composant)
   ‚îî‚îÄ> useEffect d√©tecte initialMessage
   ‚îî‚îÄ> fetcher.submit({ message, model }, { action: '/api/chat' })

4. api.chat.ts
   ‚îî‚îÄ> Parse la requ√™te
   ‚îî‚îÄ> V√©rifie le cache
   ‚îî‚îÄ> Pr√©pare les messages (system + user)
   ‚îî‚îÄ> Appelle aiService.chat({ messages, model })

5. ai-service.ts
   ‚îî‚îÄ> chat() ‚Üí d√©termine le provider (openrouter)
   ‚îî‚îÄ> chatWithOpenRouter() ‚Üí fetch vers OpenRouter API
   ‚îî‚îÄ> Retourne { content, usage, cost, ... }

6. api.chat.ts
   ‚îî‚îÄ> Retourne json({ response: aiResponse.content, ... })

7. chat.tsx
   ‚îî‚îÄ> useEffect re√ßoit fetcher.data
   ‚îî‚îÄ> Ajoute le message assistant √† l'interface
   ‚îî‚îÄ> Affiche la r√©ponse dans le chat
```

---

## ‚ö†Ô∏è POINTS IMPORTANTS

1. **Le Workbench n'est PAS encore connect√©** : 
   - Le composant `<Workbench />` est pr√©sent (ligne 209)
   - Mais les messages ne sont PAS pars√©s pour extraire le code/artefacts
   - Il manque l'utilisation de `useMessageParser` pour d√©tecter les blocs de code

2. **Pas de streaming** :
   - `stream: false` dans l'appel OpenRouter
   - La r√©ponse arrive en une seule fois

3. **Pas de persistance** :
   - Les messages sont stock√©s dans `useState` (m√©moire locale)
   - Pas de sauvegarde dans une base de donn√©es ou localStorage

4. **Cache activ√©** :
   - Les r√©ponses identiques sont mises en cache (TTL: 1 heure)
   - √âconomise les appels API

---

## üöÄ AM√âLIORATIONS POSSIBLES

1. **Parser les messages** pour extraire le code et l'afficher dans le Workbench
2. **Streaming** pour afficher la r√©ponse au fur et √† mesure
3. **Persistance** des conversations
4. **Gestion d'erreurs** plus robuste avec retry automatique

